---
title: "Class 10: Statistical Analysis - Regression"
execute:
  echo: true
  eval: true
  message: false
  warning: false
---

```{r}
#| echo: false
#| eval: true

source(here::here("prerender.R"))

```

This class will cover how to perfom different types of regression models and generate tables and figures of those models.

# Setup an R Project

Create an R Project (if you don't have one already for this course) with at least a **data** folder

üìÅ analyses

üìÅ data

# Setup Quarto Document

To follow along, you should create an empty Quarto document for this class.

## YAML

```         
---
title: "Document Title"
author: Your Name
date: today
theme: default
format:
  html:
    code-fold: true
    code-tools: true
    code-link: true
    toc: true
    toc-depth: 1
    toc-location: left
    page-layout: full
    df-print: paged
execute:
  error: true
  warning: true
self-contained: true
editor_options: 
  chunk_output_type: console
---
```

## Headers

1.  Create a level 1 header for a Setup section to load packages and set some theme options:

```         
# Setup
```

2.  Create a level 2 header to load packages

```         
## Required Packages
```

3.  Create another level 2 header where we can set a `ggplot2` theme

```         
## Plot Theme
```

## R Code Chunks

4.  Add an R code chunk below the **Required Packages** header and load the following packages,

```{r}
library(here)
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(sjPlot)
library(correlation)
library(broom)
library(performance)
library(parameters)
library(sandwich)
#library(mediation) we will use as mediation::mediate() instead of loading here
library(semTools)
library(ggeffects)
library(e1071)
library(modeloutput)
library(semoutput)
```

5.  Add an R code chunk below the **Plot Theme** header and set your own `ggplot2` theme to automatically be used in the rest of the document.

In this example I am using the `theme_classic()` settings in addition to **bolding** the title elements. This custom theme will now automatically be applied to all `ggplot2` objects by using `theme_set()`

```{r}
custom_theme <- theme_classic() +
  theme(axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold"),
        legend.title = element_text(face = "bold"),
        plot.title = element_text(face = "bold"))

theme_set(custom_theme)
```

# Example Data Set

> Suppose we have a data set in which we are interested in what variables can predict people's self-reported levels of **happiness**, measured using valid and reliable questionnaires. We measured a number of other variables we thought might best predict happiness.
>
> -   **Financial wealth** (annual income)
>
> -   **Emotional Regulation** (higher values indicate better emotional regulation ability)
>
> -   **Social Support Network** (higher values indicate a stronger social support network)
>
> -   **Chocolate Consumption** (higher values indicate more chocolate consumption on a weekly basis)
>
> -   **Type of Chocolate** (the type of chocolate they consume the most of)

You can download the data for this hypothetical experiment and save it in your **data** folder:

‚¨áÔ∏è <a href="/data/happiness_data.csv" download="happiness_data.csv">happiness_data.csv</a>

# Import Data

1.  Create a level 1 header for importing and getting the data ready for statistical analysis

```         
# Data
```

2.  Create a tabset with a tab to import the data and another tab to get the data ready

You can add multiple tabs easily by going to

> In the toolbar: Insert -\> Tabset...

```         
::: panel-tabset
## Import Data

## Get Data Ready for Models

:::
```

3.  Create an R code chunk below the **Import Data** tab header

```{r}
data_import <- read_csv(here("data", "happiness_data.csv"))
```

```{r}
#| echo: false

data_import

```

## Specify Factor Levels

Notice that the `Type of Chocolate` variable is categorical. When dealing with categorical variables for statistical analyses in R, it is usually a good idea to define the order of the categories as this will by default determine which category is treated as the reference (comparison group).

Let's set factor levels for **Type of Chocolate**

*Remember you can use* `colnames()` *to get the columns in a data frame and* `unique()` *to evaluate the unique values in a column.*

4.  Create an R code chunk below the **Get Data Ready for Models** tab header

```{r}
happiness_data <- data_import |>
  mutate(Type_of_Chocolate = factor(Type_of_Chocolate,
                                    levels = c("White", "Milk",
                                               "Dark", "Alcohol")))
```

::: callout-note
From here on out you can create your own header and tabsets as you see fit
:::

# Descriptives

It is a good idea to get a look at the data before running your analysis. One way of doing so is to calculate summary statistics (descriptives) and print it out in a table. There are some column we can't or don't want descriptives for. For instance there is usually a subject id column in data files or some variables might be categorical rather than continuous variables. We can remove those column using `select()` from the `dplyr` package.

::: panel-tabset
## Manual

```{r}
happiness_data |>
  select(-Subject, -Type_of_Chocolate) |>
  pivot_longer(everything(),
               names_to = "Variable", 
               values_to = "value") |>
  summarise(.by = Variable,
            n = length(which(!is.na(value))),
            Mean = mean(value, na.rm = TRUE),
            SD = sd(value, na.rm = TRUE),
            min = min(value, na.rm = TRUE),
            max = max(value, na.rm = TRUE),
            Skewness =
              skewness(value, na.rm = TRUE, type = 2),
            Kurtosis =
              kurtosis(value, na.rm = TRUE, type = 2),
            '% Missing' =
              100 * (length(which(is.na(value))) / n()))
```

::: callout-note
`skewness()` and `kurtosis()` are from the `e1071` package
:::

## modeloutput

I have an R package, `modeloutput` , for easily creating nice looking tables for statistical analyses. For a descriptives table we can use `descriptives_table()`.

```{r}
happiness_data |>
  select(-Subject, -Type_of_Chocolate) |>
  descriptives_table()
```
:::

# Historgrams

Visualizing the actual distribution of values on each variable is a good idea too. This is how outliers or other problematic values can be detected.

::: panel-tabset
## r plot

The most simple way to do so is by plotting a histogram for each variable in the data file using `hist()`.

```{r}
hist(happiness_data$Happiness)
```

## ggplot2

Alternatively, you can use `ggplot2()`

```{r}
ggplot(happiness_data, aes(Happiness)) +
  geom_histogram(binwidth = 10, fill = "gray", color = "black")
```
:::

# Correlation Tables

Regression models are based on the co-variation (or correlation) between variables. Therefore, we might want to first evaluate all of the pairwise correlations in the data.

::: panel-tabset
## correlation

The `correlation` package has a convenient function for creating a table of correlations that includes confidence intervals, t-values, df, and p.

```{r}
happiness_data |>
  select(-Subject, -Type_of_Chocolate) |>
  correlation() |>
  select(-CI, -Method)
```

## Heat map

Or you can the `correlation` package to create a heat map

```{r}
happiness_data |>
  select(-Subject, -Type_of_Chocolate) |>
  correlation() |>
  summary(redundant = TRUE) |>
  plot()
```

## sjPlot

The `sjPlot` package has a convenient function for creating a correlation table/matrix

```{r}
happiness_data |>
  select(-Subject, -Type_of_Chocolate) |>
  tab_corr(na.deletion = "pairwise", digits = 2, triangle = "lower")
```
:::

# Scatterplots

::: panel-tabset
## R plot

Using the base R `plot()` function

```{r}
plot(happiness_data$Financial_Wealth, happiness_data$Happiness)
```

## ggplot2

Using `ggplot2`

```{r}
ggplot(happiness_data, aes(Financial_Wealth, Happiness)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Financial Wealth (annual income)")
```

## correlation

Using the `correlation` package

```{r}
plot(cor_test(happiness_data, "Financial_Wealth", "Happiness"))
```
:::

# Regression

Regression models allow us to test more complicated relationships between variables. It is important to match your research question with the correct regression model. But also, thinking about the different types of regression models can help you clarify your research question. We will take a look at how to conduct the following regression models in R:

-   Simple regression

-   Multiple regression

-   Hierarchical regression

-   Mediation analysis

-   Moderation analysis

To conduct a regression model is fairly simple in R, all you need is to specify the regression formula in the `lm()` function.

# Simple Regression

In what is called simple regression, there is only one predictor variable. Simple regression is not really necessary because it is directly equivalent to a correlation. However, it is useful to demonstrate what happens once you add multiple predictors into a model.

The formula in the `lm()` function takes on the form of: `dv ~ iv`.

Model

```{r}
reg_simple <- lm(Happiness ~ Financial_Wealth, data = happiness_data)
```

::: panel-tabset
## R output

For regression models we can use the `summary()` call to print model results

```{r}
summary(reg_simple)
```

## easystats

The `broom` and `performance` packages provide an easy way to get model results into a table format

First using `broom::glance()` to get an overall model summary

```{r}
glance(reg_simple)
```

Then `performance::model_parameters()` to get unstandardized and standardized coefficient tables

```{r}
model_parameters(reg_simple)
```

```{r}
model_parameters(reg_simple, standardize = "refit")
```

## modeloutput

My `modeloutput` package provides a way to display regression tables in output format similar to other statistical software packages like JASP or SPSS.

```{r}
regression_tables(reg_simple)
```
:::

# Multiple Regression

You can and will most likely want to include multiple **predictor variables** in a regression model. There will be a different interpretation of the beta coefficients compared to the simple regression model. Now, the beta coefficients will represent a **unique** contribution (controlling for all the other predictor variables) of that variable to the prediction of the dependent variable. We can compare the relative strengths of the beta coefficients to decide which variable(s) are the strongest predictors of the dependent variable.

In this case, we can go ahead and include all the continuous predictor variables in a model predicting **Happiness** and compare it with the simple regression when we only had **Financial Wealth** in the model.

Model

```{r}
reg_multiple <- lm(Happiness ~ Financial_Wealth + Emotion_Regulation + 
                     Social_Support + Chocolate_Consumption,
                   data = happiness_data)
```

::: panel-tabset
## R output

For regression models we can use the `summary()` call to print model results

```{r}
summary(reg_multiple)
```

## easystats

The `broom` and `performance` packages provide an easy way to get model results into a table format

First using `broom::glance()` to get an overall model summary

```{r}
glance(reg_multiple)
```

Then `performance::model_parameters()` to get unstandardized and standardized coefficient tables

```{r}
model_parameters(reg_multiple)
```

```{r}
model_parameters(reg_multiple, standardize = "refit")
```

## modeloutput

My `modeloutput` package provides a way to display regression tables in output format similar to other statistical software packages like JASP or SPSS.

```{r}
regression_tables(reg_multiple)
```
:::

# Hierarchical Regression

Hierarchical regression is a multiptle regression approach where you compare multiple models with each other. We can compare the simple regression model we conducted above with only `Financial_Wealth` as a predictor of `Happiness` with the multiple regression model in which we added the other predictors.

::: panel-tabset
## easystats

We can use `performance:compare_performance()` to do model comparisons (hierarchical regression)

```{r}
compare_performance(reg_simple, reg_multiple)
```

## modeloutput

In `modeloutput::regression_tables()` you can add up to three models to compare

```{r}
regression_tables(reg_simple, reg_multiple)
```
:::

# Mediation

Mediation analysis allows us to test whether the effect of a predictor variable, on the dependent variable, "goes through" a mediator variable. This effect that "goes through" the mediator variable is called the indirect effect; whereas any effect of the predictor variable that does not "go through" the mediator is called the direct effect where:

**Total Effect = Direct Effect + Indirect Effect**

We are parsing the variance explained by the predictor variable into direct and indirect effects. Here are some possibilities for the outcome:

-   **Full Mediation**: The total effect of the predictor variable can be fully explained by the indirect effect. The mediator fully explains the effect of the predictor variable on the dependent variable. Direct effect = 0, indirect effect != 0.

-   **Partial Mediation**: The total effect of the predictor variable can be partially explained by the indirect effect. The mediator only partially explains the effect of the predictor variable on the dependent variable. Direct effect != 0, indirect effect != 0.

-   **No Mediation**: The total effect of the predictor variable can not be explained by the indirect effect; there is no indirect effect at all. Direct effect != 0, indirect effect = 0.

There are two general ways to perform a mediation analysis in R.

-   The standard regression method

-   Path analysis

## Regression

The regression method requires multiple regression models because we have two dependent variables in the mode - the mediation and the outcome measure. The mediator acts as both an independent variable and dependent variable in the model. Because regression requires only one dependent variable, we cannot test the full model in one regression model. Instead, we have to test the different pieces of the model one at a time. We also need to perform a test on the **indirect effect** itself, which will require a separate model.

In order to evaluate the standardize coefficients (which is often times most useful), we need to first convert the the variables into z-scores. This is fairly straightforward but does require some extra code. A z-score is standardized score on the scale of standard deviation units and is calculated as:

**z-score = (raw score - mean) / sd**

We can create new columns representing the z-scores using `mutate()` from the `dplyr` package.

```{r}
data_mediation <- happiness_data %>%
  mutate(Happiness_z = 
           (Happiness - mean(Happiness)) / sd(Happiness),
         Social_Support_z = 
           (Social_Support - mean(Social_Support)) / sd(Social_Support),
         Emotion_Regulation_z = 
           (Emotion_Regulation - mean(Emotion_Regulation)) / sd(Emotion_Regulation))
```

The standard regression model typically involves three regression models and a model testing the indirect effect.

1.  A model with the total effect: `dv ~ iv`

```{r}
reg_total <- lm(Happiness_z ~ Social_Support_z, 
                data = data_mediation)
```

2.  A model with the effect of the predictor on the mediator: `m ~ iv`

```{r}
reg_med <- lm(Emotion_Regulation_z ~ Social_Support_z, 
              data = data_mediation)
```

3.  A model with the direct effect and the effect of the mediator: `dv ~ iv + m`

```{r}
reg_direct <- lm(Happiness_z ~ Emotion_Regulation_z + Social_Support_z, 
                 data = data_mediation)
```

4.  A statistical test of the indirect effect: `mediation::mediate()`

```{r}
model_mediation <- mediation::mediate(reg_med, reg_direct,
                                      treat = "Social_Support_z",
                                      mediator = "Emotion_Regulation_z",
                                      boot = TRUE, sims = 1000, 
                                      boot.ci.type = "bca")
```

::: panel-tabset
## easystats

### 1. DV \~ IV

```{r}
glance(reg_total)

model_parameters(reg_total, standardize = "refit")
```

### 2. M \~ IV

```{r}
glance(reg_med)

model_parameters(reg_med, standardize = "refit")
```

### 3. DV \~ IV + M

```{r}
glance(reg_direct)

model_parameters(reg_direct, standardize = "refit")
```

### 4. Mediation

```{r}
model_parameters(model_mediation)
```

## modeloutput

### Regression Tables

```{r}
regression_tables(reg_total, reg_direct)
```

```{r}
regression_tables(reg_med)
```

### Mediation

```{r}
model_parameters(model_mediation)
```
:::

## Path Analysis

There is a statistical modelling approach called **path analysis** that does allow for multiple dependent variables in a single model. Here is how to specify a path analysis model using the `lavaan` package.

See the [lavaan website](https://lavaan.ugent.be/tutorial/mediation.html){target="_blank"} for how to conduct mediation in lavaan

Model

```{r}
model <- "
# a path
Emotion_Regulation ~ a*Social_Support

# b path
Happiness ~ b*Emotion_Regulation

# c prime path 
Happiness ~ c*Social_Support

# indirect and total effects
indirect := a*b
total := c + a*b
"

fit <- sem(model, data = happiness_data)
```

::: panel-tabset
## R output

```{r}
summary(fit)
```

## semoutput

I also have a package, `semoutput` for displaying results from `lavaan` models:

```{r}
sem_tables(fit)
```
:::

It is highly advised to calculate bootstrapped and bias-corrected confidence intervals around the indirect effect:

```{r}
monteCarloCI(fit, nRep = 1000, standardized = TRUE)
```

# Moderation

Another type of multiple regression model is the **moderation** analysis. The concept of moderation in regression is analogous to the concept of an interaction in ANOVA. The idea here is that the relationship (correlation or slope) between two variables changes as a function of another variable, called the **moderator**. As the values on the moderator variable increases, the correlation or slope might increase or decrease. The moderator can be either continuous or categorical.

In our happiness data, we can test whether the relationship between chocolate consumption and happiness is moderated by the type of chocolate consumed. In this case, type of chocolate is a categorical variable: White, milk, dark, or alcohol.

**It is highly advisable to first "center" the predictor and moderator variables. Centering refers to centering the scores around the mean (making the mean = 0). This can simply be done by subtracting out the mean from each score. We can do this with** `mutate()` **from the** `dplyr` **package.**

Since type of chocolate is categorical we do not need to center it.

```{r}
data_mod <- happiness_data %>%
  mutate(Chocolate_Consumed_c = 
           Chocolate_Consumption - mean(Chocolate_Consumption))
```

Model

```{r}
model_null <- lm(Happiness ~ Chocolate_Consumed_c + Type_of_Chocolate, 
                 data = data_mod)

model_moderation <- lm(Happiness ~ Chocolate_Consumed_c * Type_of_Chocolate, 
                       data = data_mod)
```

::: panel-tabset
## R output

```{r}
summary(model_null)

summary(model_moderation)
```

## easystats

### Null Model

```{r}
glance(model_null)

model_parameters(model_null, standardize = "refit")
```

### Interaction

```{r}
glance(model_moderation)

model_parameters(model_moderation, standardize = "refit")
```

## modeloutput

```{r}
regression_tables(model_null, model_moderation)
```
:::

To better understand the nature of a moderation, it helps to plot the results.

::: panel-tabset
## sjPlot

The easiest way to plot the results of a moderation analysis is using the `plot_model()` function from the `sjPlot` package. Note that it is a bit more complicated to plot a moderation analysis when the moderator is a continuous variable instead of categorical but the `sjPlot` package actually makes it a bit easier.

```{r}
plot_model(model_moderation, typ = "int")
```

## ggeffects + ggplot2

The most customizable way to plot model results is using the `ggeffects` and `ggplot2` packages. We can use `ggeffect()` to get a data frame with estimated model values for specified terms in the model and the dependent variable.

For continuous variables, we can customize at what values we want to plot. For "all" possible values from the original data we can specify `[all]`. If your variable was standardized before entering it into the model you can use `[2, 0, -2]` to get values at +2 standard deviations above the mean, the mean, and -2 standard deviations below the mean.

```{r}
data_plot <- ggeffect(model_moderation,
                       terms = c("Chocolate_Consumed_c [all]", 
                                 "Type_of_Chocolate")) |>
  rename(Chocolate_Consumed_c = x, Type_of_Chocolate = group,
         Happiness = predicted)
```

```{r}
#| echo: false

data_plot

```

```{r}
ggplot(data_plot, aes(Chocolate_Consumed_c, Happiness, 
                      color = Type_of_Chocolate)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, 
                  fill = Type_of_Chocolate), 
              show.legend = FALSE, alpha = .1, color = NA) +
  geom_smooth(method = "lm", alpha = 0) +
  scale_color_brewer(palette = "Set1",
                     guide_legend(title = "Type of Chocolate")) +
  labs(x = "Amount of Chocolate Consumed")
```
:::
